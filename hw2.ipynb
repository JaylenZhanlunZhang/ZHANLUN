{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e2bd0e-3941-42a6-9420-e7379230e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataframe columns: Index(['Question', 'Coarse', 'Fine'], dtype='object')\n",
      "Total data size after filtering: 4994\n",
      "Training set size: 3994\n",
      "Validation set size: 300\n",
      "Test set size: 700\n",
      "Data has been saved to the specified directory.\n",
      "Tokenizer: facebook/opt-350m\n",
      "Vocabulary size: 50265\n",
      "Model parameters: 331196416\n",
      "Model max length: 2048\n",
      "Starting zero-shot testing...\n",
      "Processed 100 samples...\n",
      "Processed 200 samples...\n",
      "Processed 300 samples...\n",
      "Processed 400 samples...\n",
      "Processed 500 samples...\n",
      "Processed 600 samples...\n",
      "Processed 700 samples...\n",
      "Zero-shot Accuracy: 0.00%\n",
      "Starting 1-shot testing...\n",
      "Processed 100 samples...\n",
      "Processed 200 samples...\n",
      "Processed 300 samples...\n",
      "Processed 400 samples...\n",
      "Processed 500 samples...\n",
      "Processed 600 samples...\n",
      "Processed 700 samples...\n",
      "1-shot Accuracy: 19.57%\n",
      "Starting 3-shot testing...\n",
      "Processed 100 samples...\n",
      "Processed 200 samples...\n",
      "Processed 300 samples...\n",
      "Processed 400 samples...\n",
      "Processed 500 samples...\n",
      "Processed 600 samples...\n",
      "Processed 700 samples...\n",
      "3-shot Accuracy: 23.29%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc1cca5a298425aae02ee41dfb1cbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5722e2049e4f4c10aa72812a05b20ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python3.11.8\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.120324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the fine-tuned model...\n",
      "Processed 100 samples...\n",
      "Processed 200 samples...\n",
      "Processed 300 samples...\n",
      "Processed 400 samples...\n",
      "Processed 500 samples...\n",
      "Processed 600 samples...\n",
      "Processed 700 samples...\n",
      "SFT Accuracy: 86.86%\n",
      "\n",
      "Final Results:\n",
      "Zero-shot Accuracy: 0.00%\n",
      "1-shot Accuracy: 19.57%\n",
      "3-shot Accuracy: 23.29%\n",
      "SFT Accuracy: 86.86%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataDir = './assignment-2/'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "\n",
    "\n",
    "# Add header=None because the CSV file does not have column names\n",
    "data = pd.read_csv('5000TravelQuestionsDataset.csv', encoding='ISO-8859-1', header=None)\n",
    "\n",
    "# Set column names\n",
    "data.columns = ['Question', 'Coarse', 'Fine']\n",
    "\n",
    "#  column names to verify\n",
    "print(\"Dataframe columns:\", data.columns)\n",
    "\n",
    "# remove fine-grained category column, keep only coarse-grained category\n",
    "data = data[['Question', 'Coarse']]\n",
    "\n",
    "\n",
    "data.columns = ['Question', 'Category']\n",
    "\n",
    "#  only specified coarse-grained categories\n",
    "allowedCategories = ['TTD', 'TGU', 'ACM', 'TRS', 'WTH', 'FOD', 'ENT']\n",
    "data = data[data['Category'].isin(allowedCategories)].reset_index(drop=True)\n",
    "\n",
    "#  dataset size\n",
    "print(f\"Total data size after filtering: {len(data)}\")\n",
    "\n",
    "# Split dataset\n",
    "trainData, tempData = train_test_split(\n",
    "    data, test_size=1000, random_state=42, stratify=data['Category']\n",
    ")\n",
    "valData, testData = train_test_split(\n",
    "    tempData, test_size=700, random_state=42, stratify=tempData['Category']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(trainData)}\")\n",
    "print(f\"Validation set size: {len(valData)}\")\n",
    "print(f\"Test set size: {len(testData)}\")\n",
    "\n",
    "#  dataset to instruction format\n",
    "def convertToInstructionFormat(df):\n",
    "    dfConverted = df.copy()\n",
    "    dfConverted['instruction'] = 'Please determine the category (TTD, TGU, ACM, TRS, WTH, FOD, ENT) based on the content of the following question:'\n",
    "    dfConverted['input'] = dfConverted['Question']\n",
    "    dfConverted['output'] = dfConverted['Category']\n",
    "    return dfConverted[['instruction', 'input', 'output']]\n",
    "\n",
    "trainDataConverted = convertToInstructionFormat(trainData)\n",
    "valDataConverted = convertToInstructionFormat(valData)\n",
    "testDataConverted = convertToInstructionFormat(testData)\n",
    "\n",
    "#  datasets to the specified directory\n",
    "trainDataConverted.to_csv(os.path.join(dataDir, 'train_data.csv'), index=False)\n",
    "valDataConverted.to_csv(os.path.join(dataDir, 'val_data.csv'), index=False)\n",
    "testDataConverted.to_csv(os.path.join(dataDir, 'test_data.csv'), index=False)\n",
    "\n",
    "print(\"Data has been saved to the specified directory.\")\n",
    "\n",
    "# Model Selection\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "modelName = 'facebook/opt-350m' \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForCausalLM.from_pretrained(modelName).to(device)\n",
    "\n",
    "\n",
    "print(f\"Tokenizer: {tokenizer.name_or_path}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model parameters: {model.num_parameters()}\")\n",
    "print(f\"Model max length: {model.config.max_position_embeddings}\")\n",
    "\n",
    "\n",
    "\n",
    "#  prompt template\n",
    "def generatePrompt(question, examples=[]):\n",
    "    prompt = \"Please determine the category (TTD, TGU, ACM, TRS, WTH, FOD, ENT) based on the content of the following question:\\n\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"Question: {ex['input']}\\nAnswer: {ex['output']}\\n\\n\"\n",
    "    prompt += f\"Question: {question}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "# Zero-shot Testing\n",
    "\n",
    "def zeroShotTest(testData):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx, row in testData.iterrows():\n",
    "        prompt = generatePrompt(row['input'])\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=10, temperature=0.7, top_p=0.9, do_sample=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        predictedCategory = response.strip().split()[0]\n",
    "        if predictedCategory == row['output']:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print(f\"Processed {total} samples...\")\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Few-shot Testing\n",
    "\n",
    "def fewShotTest(testData, k):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    examples = trainDataConverted.sample(k).to_dict('records')\n",
    "    for idx, row in testData.iterrows():\n",
    "        prompt = generatePrompt(row['input'], examples)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=10, temperature=0.7, top_p=0.9, do_sample=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        predictedCategory = response.strip().split()[0]\n",
    "        if predictedCategory == row['output']:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print(f\"Processed {total} samples...\")\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "#  Accuracy\n",
    "\n",
    "print(\"Starting zero-shot testing...\")\n",
    "zeroShotAccuracy = zeroShotTest(testDataConverted)\n",
    "print(f\"Zero-shot Accuracy: {zeroShotAccuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"Starting 1-shot testing...\")\n",
    "oneShotAccuracy = fewShotTest(testDataConverted, k=1)\n",
    "print(f\"1-shot Accuracy: {oneShotAccuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"Starting 3-shot testing...\")\n",
    "threeShotAccuracy = fewShotTest(testDataConverted, k=3)\n",
    "print(f\"3-shot Accuracy: {threeShotAccuracy * 100:.2f}%\")\n",
    "\n",
    "# Supervised Fine-Tuning SFT\n",
    "\n",
    "# Prepare fine-tuning data\n",
    "trainDataset = Dataset.from_pandas(trainDataConverted)\n",
    "valDataset = Dataset.from_pandas(valDataConverted)\n",
    "\n",
    "#  data processing function\n",
    "def preprocessFunction(examples):\n",
    "    # Construct prompt\n",
    "    prompts = [\n",
    "        instruction + '\\nQuestion: ' + inp + '\\nAnswer:'\n",
    "        for instruction, inp in zip(examples['instruction'], examples['input'])\n",
    "    ]\n",
    "    # Construct answers\n",
    "    answers = [out + tokenizer.eos_token for out in examples['output']]\n",
    "\n",
    "    # Encode prompts and answers\n",
    "    tokenizedInputs = tokenizer(\n",
    "        prompts,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "    )\n",
    "    tokenizedAnswers = tokenizer(\n",
    "        answers,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=10,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    # Concatenate inputs  labels\n",
    "    inputIds = []\n",
    "    labels = []\n",
    "    for inputIdsPart, answerIds in zip(tokenizedInputs['input_ids'], tokenizedAnswers['input_ids']):\n",
    "        inputId = inputIdsPart + answerIds\n",
    "        label = [-100] * len(inputIdsPart) + answerIds\n",
    "\n",
    "        # Truncate to maximum length\n",
    "        inputId = inputId[:512]\n",
    "        label = label[:512]\n",
    "\n",
    "        inputIds.append(inputId)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Return unpadded sequences\n",
    "    return {'input_ids': inputIds, 'labels': labels}\n",
    "\n",
    "# Process datasets\n",
    "trainDataset = trainDataset.map(preprocessFunction, batched=True, remove_columns=trainDataset.column_names)\n",
    "valDataset = valDataset.map(preprocessFunction, batched=True, remove_columns=valDataset.column_names)\n",
    "\n",
    "#  custom data collator\n",
    "def dataCollator(features):\n",
    "    inputIds = [torch.tensor(f['input_ids'], dtype=torch.long) for f in features]\n",
    "    labels = [torch.tensor(f['labels'], dtype=torch.long) for f in features]\n",
    "\n",
    "    # Pad sequences\n",
    "    inputIds = pad_sequence(inputIds, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    # Generate attention_mask\n",
    "    attentionMask = (inputIds != tokenizer.pad_token_id).long()\n",
    "\n",
    "    return {'input_ids': inputIds, 'labels': labels, 'attention_mask': attentionMask}\n",
    "\n",
    "# Set training parameters\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # You can increase the number of training epochs as needed\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),  # Use fp16 if available\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "#  Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=trainDataset,\n",
    "    eval_dataset=valDataset,\n",
    "    data_collator=dataCollator,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "print(\"Starting model fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "\n",
    "def sftTest(testData):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for idx, row in testData.iterrows():\n",
    "        prompt = generatePrompt(row['input'])\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=10, temperature=0.7, top_p=0.9, do_sample=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        predictedCategory = response.strip().split()[0]\n",
    "        if predictedCategory == row['output']:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if total % 100 == 0:\n",
    "            print(f\"Processed {total} samples...\")\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"Evaluating the fine-tuned model...\")\n",
    "sftAccuracy = sftTest(testDataConverted)\n",
    "print(f\"SFT Accuracy: {sftAccuracy * 100:.2f}%\")\n",
    "\n",
    "# Final Results\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Zero-shot Accuracy: {zeroShotAccuracy * 100:.2f}%\")\n",
    "print(f\"1-shot Accuracy: {oneShotAccuracy * 100:.2f}%\")\n",
    "print(f\"3-shot Accuracy: {threeShotAccuracy * 100:.2f}%\")\n",
    "print(f\"SFT Accuracy: {sftAccuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f41552-6ba5-4f37-8b84-6be693823727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
